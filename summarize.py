# 1. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
!pip install transformers sentencepiece torch --quiet

# 2. íŒŒì¼ ì—…ë¡œë“œ
from google.colab import files
uploaded = files.upload()  # ğŸ‘‰ ì—¬ê¸°ì„œ íŒŒì¼ ì„ íƒí•˜ì„¸ìš”

input_file = list(uploaded.keys())[0]  # ì—…ë¡œë“œëœ íŒŒì¼ ì´ë¦„

# 3. ëª¨ë¸ ë¡œë”© (KoBART - ê¸´ ìš”ì•½ìš©)
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "digit82/kobart-summarization"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)

# 4. ìš”ì•½ í•¨ìˆ˜ ì •ì˜
def generate_long_summary_kobart(text):
    inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True, padding=True).to(device)
    output = model.generate(
        inputs["input_ids"],
        max_length=90,
        min_length=70,
        num_beams=4,
        repetition_penalty=2.0,
        no_repeat_ngram_size=3,
        early_stopping=True
    )
    return tokenizer.decode(output[0], skip_special_tokens=True)

# 5. íŒŒì¼ ë¶„í•  & ì²˜ë¦¬ í•¨ìˆ˜
import json
import os

def process_jsonl_file(input_path, start_index=0, end_index=None):
    with open(input_path, 'r', encoding='utf-8') as infile:
        lines = infile.readlines()

    if end_index is None:
        end_index = len(lines)

    lines_to_process = lines[start_index:end_index]

    # ì¶œë ¥ íŒŒì¼ëª… ìë™ ìƒì„±
    output_file = f"long_summary_{start_index}_{end_index or 'end'}.jsonl"
    
    # íŒŒì¼ ëª¨ë“œ ì„¤ì •: ì²˜ìŒì´ë©´ 'w', ìˆìœ¼ë©´ 'a'
    file_mode = 'a' if os.path.exists(output_file) else 'w'

    with open(output_file, file_mode, encoding='utf-8') as outfile:
        for idx, line in enumerate(lines_to_process, start=start_index):
            try:
                obj = json.loads(line)
                original_text = obj.get("text") or obj.get("input")
                if not original_text:
                    continue
                summary_long = generate_long_summary_kobart(original_text)
                obj["summary_long"] = summary_long
                outfile.write(json.dumps(obj, ensure_ascii=False) + "\n")
            except Exception as e:
                print(f"âŒ {idx}ë²ˆì§¸ ì¤„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue

    print(f"âœ… {output_file} ì €ì¥ ì™„ë£Œ!")
    return output_file

# 6. ì‹¤í–‰ ì˜ˆì‹œ (ì›í•˜ëŠ” êµ¬ê°„ë§Œ ìˆ˜ì •í•˜ë©´ ë¨)
output_file = process_jsonl_file(input_file, start_index=0, end_index=100)

# 7. ë‹¤ìš´ë¡œë“œ
files.download(output_file)
