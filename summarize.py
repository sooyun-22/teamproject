# 1. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
!pip install transformers sentencepiece torch --quiet

# 2. íŒŒì¼ ì—…ë¡œë“œ
from google.colab import files
uploaded = files.upload()  # ğŸ‘‰ ì—¬ê¸°ì„œ íŒŒì¼ ì„ íƒí•˜ì„¸ìš”

input_file = list(uploaded.keys())[0]  # ì—…ë¡œë“œëœ íŒŒì¼ ì´ë¦„
output_file = "long_summary_output.jsonl"

# 3. ëª¨ë¸ ë¡œë”© (KoBART - ê¸´ ìš”ì•½ìš©)
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "digit82/kobart-summarization"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)

# 4. ìš”ì•½ í•¨ìˆ˜ ì •ì˜
def generate_long_summary_kobart(text):
    inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True, padding=True).to(device)
    output = model.generate(
        inputs["input_ids"],
        max_length=90,
        min_length=70,
        num_beams=4,
        repetition_penalty=2.0,
        no_repeat_ngram_size=3,
        early_stopping=True
    )
    return tokenizer.decode(output[0], skip_special_tokens=True)

# 5. íŒŒì¼ ë¶„í•  & ì²˜ë¦¬
import json

def process_jsonl_file(input_path, output_path, chunk_size=100):
    with open(input_path, 'r', encoding='utf-8') as infile:
        lines = infile.readlines()

    with open(output_path, 'w', encoding='utf-8') as outfile:
        for i in range(0, len(lines), chunk_size):
            chunk = lines[i:i+chunk_size]
            for line in chunk:
                try:
                    obj = json.loads(line)
                    original_text = obj.get("text") or obj.get("input")
                    if not original_text:
                        continue
                    summary_long = generate_long_summary_kobart(original_text)
                    obj["summary_long"] = summary_long
                    outfile.write(json.dumps(obj, ensure_ascii=False) + "\n")
                except Exception as e:
                    print(f"âŒ ì˜¤ë¥˜ ë°œìƒ (ê±´ë„ˆëœ€): {e}")
                    continue

# 6. ì‹¤í–‰
process_jsonl_file(input_file, output_file)
print("âœ… ê¸´ ìš”ì•½ ìƒì„± ì™„ë£Œ!")

# 7. ë‹¤ìš´ë¡œë“œ ë§í¬ ì œê³µ
files.download(output_file)
